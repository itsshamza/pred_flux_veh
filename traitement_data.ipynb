{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traiter et fusionner des fichiers CSV horaires en regroupant et en complétant les données manquantes par des moyennes et des interpolations, puis enregistrer les résultats sous forme de fichiers CSV pour les directions entrantes et sortantes.\n",
    "\n",
    " Ajouter des colonnes de date dérivées et renommer les colonnes pour assurer la cohérence des formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_and_merge_files(input_folder, start_date, end_date):\n",
    "    all_dfs = []      \n",
    "    \n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            \n",
    "\n",
    "            df = pd.read_csv(file_path, parse_dates=['Datetime'])\n",
    "            \n",
    "            df = df.groupby(['Datetime', 'Sens'], as_index=False).agg({\n",
    "                '2R': 'sum', 'VL': 'sum', 'PL': 'sum', 'DayOfWeek': 'first'\n",
    "            })\n",
    "            \n",
    "            df['Hour'] = df['Datetime'].dt.hour\n",
    "            df['DayOfMonth'] = df['Datetime'].dt.day\n",
    "            \n",
    "            full_range = pd.DataFrame(date_range, columns=['Datetime'])\n",
    "            full_range['DayOfWeek'] = full_range['Datetime'].dt.dayofweek\n",
    "            full_range['Hour'] = full_range['Datetime'].dt.hour\n",
    "            full_range['DayOfMonth'] = full_range['Datetime'].dt.day\n",
    "            full_range['Month'] = full_range['Datetime'].dt.month\n",
    "\n",
    "            full_range_sens = pd.concat([full_range.assign(Sens=0), full_range.assign(Sens=1)])\n",
    "\n",
    "            df = pd.merge(full_range_sens, df, on=['Datetime', 'DayOfWeek', 'Hour', 'DayOfMonth', 'Sens'], how='left')\n",
    "\n",
    "            # Remplir les valeurs manquantes en utilisant la moyenne par DayOfWeek et Hour\n",
    "            df[['2R', 'VL', 'PL']] = df.groupby(['DayOfWeek', 'Hour','Sens'])[['2R', 'VL', 'PL']].transform(lambda x: x.fillna(x.mean()))\n",
    "            \n",
    "            nan_values = df[['2R', 'VL', 'PL']].isna()\n",
    "            if nan_values.any().any():\n",
    "                nan_rows = df[nan_values.any(axis=1)]\n",
    "                print(f\"\\nNaN values detected in the following rows in file: {filename}\")\n",
    "                print(nan_rows[['Datetime', 'DayOfWeek', 'Hour', 'DayOfMonth', 'Sens', '2R', 'VL', 'PL']])\n",
    "\n",
    "                # Interpolation linéaire pour combler les NaN\n",
    "                df[['2R', 'VL', 'PL']] = df[['2R', 'VL', 'PL']].interpolate(method='linear', limit_direction='both')\n",
    "                nan_values_after_interp = df[['2R', 'VL', 'PL']].isna()\n",
    "\n",
    "                if nan_values_after_interp.any().any():\n",
    "                    print(f\"\\nNaN values remaining after interpolation in file: {filename}\")\n",
    "                    nan_rows_after_interp = df[nan_values_after_interp.any(axis=1)]\n",
    "                    print(nan_rows_after_interp[['Datetime', 'DayOfWeek', 'Hour', 'DayOfMonth', 'Sens', '2R', 'VL', 'PL']])\n",
    "                else:\n",
    "                    print(f\"\\nNaN values successfully filled by interpolation in file: {filename}\")\n",
    "\n",
    "\n",
    "            # Convertir les colonnes 2R, VL, PL en entiers\n",
    "            df[['2R', 'VL', 'PL']] = df[['2R', 'VL', 'PL']].round().astype(int)\n",
    "\n",
    "            post_number = filename.split('.')[0].replace(\"P\", \"\").zfill(2)  # S'assurer que le numéro de poste a deux chiffres\n",
    "            \n",
    "            # Renommer les colonnes avec le bon format\n",
    "            df.rename(columns={\n",
    "                '2R': f'2R-P{post_number}', \n",
    "                'VL': f'VL-P{post_number}', \n",
    "                'PL': f'PL-P{post_number}'}, inplace=True)\n",
    "            \n",
    "            all_dfs.append(df)\n",
    "\n",
    "    \n",
    "    # Fusionner tous les DataFrames par Datetime, DayOfWeek, Hour, DayOfMonth, et Sens\n",
    "    merged_df = all_dfs[0]\n",
    "    for df in all_dfs[1:]:\n",
    "        merged_df = pd.merge(merged_df, df, on=['Datetime', 'DayOfWeek', 'Hour', 'DayOfMonth','Month', 'Sens'], how='outer')\n",
    "    \n",
    "    df_entrante = merged_df[merged_df['Sens'] == 0].drop(columns=['Sens'])\n",
    "    df_sortante = merged_df[merged_df['Sens'] == 1].drop(columns=['Sens'])\n",
    "    # Sauvegarder le fichier fusionné final\n",
    "    df_entrante.to_csv('3month_data_en.csv', index=False)\n",
    "    df_sortante.to_csv('3month_data_so.csv', index=False)\n",
    "    print(f\"Final merged file saved \")\n",
    "\n",
    "input_folder = './3types'  \n",
    "start_date = '2022-10-03' \n",
    "end_date = '2023-01-03'  \n",
    "\n",
    "process_and_merge_files(input_folder, start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enrichir un fichier de données de trafic avec des données météorologiques en fusionnant les deux ensembles de données sur la colonne Datetime et en complétant les valeurs manquantes par interpolation. Sauvegarder le fichier enrichi avec un ensemble de colonnes ordonnées dans un fichier CSV de sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from meteostat import Point, Hourly\n",
    "\n",
    "def enrich_with_meteo(input_file, output_file, location):\n",
    "    # Lire le fichier d'entrée\n",
    "    df = pd.read_csv(input_file, parse_dates=['Datetime'])\n",
    "    \n",
    "    # Extraire la plage de dates du fichier\n",
    "    start_date = df['Datetime'].min()\n",
    "    end_date = df['Datetime'].max()\n",
    "\n",
    "    # Extraire les données météo pour l'intervalle de dates spécifié\n",
    "    meteo_data = Hourly(location, start_date, end_date).fetch()\n",
    "\n",
    "    if meteo_data.empty:\n",
    "        print(\"Les données météorologiques sont vides. Assurez-vous que les dates et l'emplacement sont corrects.\")\n",
    "        return\n",
    "\n",
    "    # Préparer les colonnes de données météorologiques\n",
    "    meteo_data['DayOfWeek'] = meteo_data.index.dayofweek\n",
    "    meteo_data['Hour'] = meteo_data.index.hour\n",
    "    meteo_data['DayOfMonth'] = meteo_data.index.day\n",
    "    \n",
    "    meteo_data.rename(columns={\n",
    "        'temp': 'temperature(degC)',\n",
    "        'dwpt': 'point_de_rosee(degC)',\n",
    "        'rhum': 'humidite(%)',\n",
    "        'prcp': 'precipitations(mm)',\n",
    "        'snow': 'neige(mm)',\n",
    "        'wdir': 'vent_direction(deg)',\n",
    "        'wspd': 'vent_moyen(km/h)',\n",
    "        'wpgt': 'rafale_vent_max(km/h)',\n",
    "        'pres': 'pression(hPa)',\n",
    "        'tsun': 'ensoleillement(h)'  # Correction pour les heures d'ensoleillement\n",
    "    }, inplace=True)\n",
    "\n",
    "    meteo_data.reset_index(inplace=True)  # Mettre l'index comme colonne 'Datetime'\n",
    "    meteo_data.rename(columns={'time': 'Datetime'}, inplace=True)  # Renommer la colonne 'time' en 'Datetime'\n",
    "\n",
    "    # Fusionner les données de trafic avec les données météorologiques\n",
    "    df_merged = pd.merge(df, meteo_data, on='Datetime', how='left')\n",
    "    if 'Hour' not in df_merged.columns:\n",
    "            df_merged['Hour'] = df_merged['Datetime'].dt.hour\n",
    "    if 'DayOfWeek' not in df_merged.columns:\n",
    "            df_merged['DayOfWeek'] = df_merged['Datetime'].dt.dayofweek\n",
    "    if 'DayOfMonth' not in df_merged.columns:\n",
    "            df_merged['DayOfMonth'] = df_merged['Datetime'].dt.day\n",
    "    if 'Month' not in df_merged.columns:\n",
    "            df_merged['Month'] = df_merged['Datetime'].dt.month\n",
    "    \n",
    "    meteo_columns = ['temperature(degC)', 'point_de_rosee(degC)', 'humidite(%)', 'precipitations(mm)', 'neige(mm)', \n",
    "                        'vent_direction(deg)', 'vent_moyen(km/h)', 'rafale_vent_max(km/h)', 'pression(hPa)', 'ensoleillement(h)']\n",
    "        \n",
    "    df_merged[meteo_columns] = df_merged[meteo_columns].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    ordered_columns = ['Datetime', 'Hour', 'DayOfWeek', 'DayOfMonth', 'Month'] + meteo_columns + [col for col in df_merged.columns if col.startswith('2R') or col.startswith('VL') or col.startswith('PL')]\n",
    "\n",
    "    df_merged = df_merged[ordered_columns]\n",
    "    df_merged.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Fichier enrichi avec les données météo enregistré avec succès : {output_file}\")\n",
    "\n",
    "input_file = './3month_data_so.csv'  # Le fichier d'entrée avec les données de trafic\n",
    "output_file = './3month_meteo_data_so.csv'  # Fichier de sortie avec les données météo ajoutées\n",
    "\n",
    "location = Point(44.8069, -0.6133, 20)\n",
    "\n",
    "enrich_with_meteo(input_file, output_file, location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant d'effectuer ça, j'ai parcouru manuellement les fichiers pour les mettres sous le meme format, et le meme intervalle.\n",
    "\n",
    "Par exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./P25complet.csv')\n",
    "\n",
    "df['2R'] = df['VELO'] + df['MOTO'] + df['EDPM']\n",
    "df['PL'] = df['PL_1'] + df['PL_2'] + df['BUS'] + df['UT']\n",
    "df = df[['Datetime', 'DayOfWeek', 'Sens', '2R', 'VL', 'PL']]\n",
    "df.to_csv('./3types/P25.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
